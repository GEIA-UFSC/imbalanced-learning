{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Imbalanced learning\n",
    "Artur Back de Luca\n",
    "\n",
    "GEIA - Grupo de estudos em Inteligência Aritificial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Content:\n",
    "\n",
    "- Defintion\n",
    "- Context\n",
    "- Problematic\n",
    "- Approaches:\n",
    "    1. Problem based approaches\n",
    "    2. Data based approaches\n",
    "    3. Algorithm based approaches\n",
    "- Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Imbalanced datasets\n",
    "\n",
    "&ensp;&ensp;&ensp;&ensp;\n",
    "_An unequal proportion of class examples_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Context\n",
    "\n",
    "Several real world problems involve some sort of class imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "<ol>\n",
    "    <li><b>Banking:</b> Fraud detection (there are less fraudulent than authentic transactions)</li>\n",
    "    <li><b>Health:</b> Disease detection (rare conditions)</li>\n",
    "    <li><b>Manufacturing:</b> Flaw detection (e.g. failed welded joint in product)</li>\n",
    "</ol>\n",
    "<br><br><br><br>\n",
    "\n",
    "<p style=\"font-size:0.8em; text-align:right\">\n",
    "<a href=\"https://pdfs.semanticscholar.org/6e19/3366945bf3bd72d5ba906e3982ac4d8ae874.pdf\">[1]</a>    : Chan, P., & Stolfo, S. (2001). Toward scalable learning with non-uniform class and cost distributions: A case study in credit card fraud detection<br>\n",
    "<a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3163175/pdf/1472-6947-11-51.pdf\">[2]</a>: Khalilia, M., Chakraborty, S., & Popescu, M. (2011). Predicting disease risks from highly imbalanced data using random forest <br>\n",
    "<a href=\"https://sci2s.ugr.es/keel/pdf/specific/articulo/liao_classification_2008.pdf\">[3]</a>: Liao, T. (2008). Classification of weld flaws with imbalanced data\n",
    "    \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Additional example: Oil Spills"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "It's important to notice that it applies solely to **classification** problems and it is only evident on **supervised** learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What are Imbalanced datasets?\n",
    "\n",
    "An unequal proportion of class examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "However, there's no such thing as a:\n",
    " - Equal set of examples\n",
    " - Standard threshold of imbalance proportion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Imbalance proportion scale\n",
    "\n",
    "There's, however, consensus regarding its severity:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " - 2:1  -  `marginally imbalanced`\n",
    " - 10:1 - `modestly imbalanced`\n",
    " - ≥100:1 - `extremely imbalanced`\n",
    " - ≥10⁶:1 - `absolute rarity`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why does it matter?\n",
    "\n",
    "<img src=\"./pictures/error_imbalance_ratio.png\" width=\"1200\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## A metric trap\n",
    "<p style=\"font-size:0.8em;\"><i>Accuracy paradox</i></p>\n",
    "\n",
    "Imagine that you have a 99:1 dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "If your model estimates that all of your dataset has the same value the predominant set:\n",
    "\n",
    "`Accuracy (ACC):` 99%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> Is this good? Most likely not. The model hasn't learned the interactions between the classes and their features, it simply learned to guess the dominant value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "> In fact, most algorithms\n",
    "are still designed and tested much more thoroughly for accuracy optimization\n",
    "than for the optimization of other evaluation metrics. This issue is impacted by\n",
    "the metrics used to guide the heuristic search process. For example, decision trees\n",
    "are generally formed in a top–down manner and the tree construction process\n",
    "focuses on selecting the best test condition to expand the extremities of the tree.\n",
    "The quality of the test condition (i.e., the condition used to split the data at the\n",
    "node) is usually determined by the “purity” of a split, which is often computed as\n",
    "the weighted average of the purity values of each branch, where the weights are\n",
    "determined by the fraction of examples that follow that branch.<br>\n",
    "> \n",
    "><p  style=\"font-size:0.8em; text-align:right\">[4] Weiss, G. M. (2013). Foundations of Imbalanced Learning. (pg. 24)</p> "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Algorithm level issues\n",
    "\n",
    "e.g.: Decision Trees\n",
    "\n",
    "<img src=\"./pictures/decision_tree.png\" width=\"750\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How to overcome?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1. Problem definition level approaches\n",
    "2. Data level approaches\n",
    "3. Algorithm level approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1. Problem definition level approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Redefine the problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<br>\n",
    "Before jumping in more sophisticated approaches, one interest attempt is working with a specific subdomain of interest, rather than the whole dataset, where the relative imbalance is lessened."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> As a simple example, in medical diagnosis, one could restrict the population to people over 90 years of age, especially if the targeted disease tends to be more common in the aged.\n",
    ">\n",
    "><p  style=\"font-size:0.8em; text-align:right\">[4] Weiss, G. M. (2013). Foundations of Imbalanced Learning. (pg. 28)</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Use Appropriate Evaluation Metrics\n",
    "\n",
    "Instead of choosing ACC and relative metrics, utilize metrics that identify the incorrect classification of imbalanced samples\n",
    "\n",
    " - Confusion Matrix\n",
    " - ROC and AUC\n",
    " - Precision and Recall\n",
    " - F-measure and the F1-score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Confusion Matrix\n",
    "\n",
    "```\n",
    "                    +------------------+----------+----------+\n",
    "                    | Predicted/Actual | Positive | Negative |\n",
    "                    +------------------+----------+----------+\n",
    "                    | Positive         |    TP    |    FP    |\n",
    "                    | Negative         |    FN    |    TN    |\n",
    "                    +------------------+----------+----------+\n",
    "```\n",
    "\n",
    "## ROC Curve\n",
    "\n",
    "$$True\\:Positive_{Rate}\\:(TPR) = \\frac{TP}{FN+TP}$$\n",
    "\n",
    "$$False\\:Positive_{Rate}\\:(FPR) = \\frac{FP}{FN+TP}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Consider in this and in the other definitions that **Positive** is the minor class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import IFrame    \n",
    "frame = IFrame('http://www.navan.name/roc/#slider', width=600, height=555)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"600\"\n",
       "            height=\"555\"\n",
       "            src=\"http://www.navan.name/roc/#slider\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f6e2c56e940>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "By interacting with the slider we are changing the hability of the model to sepate the two analyzed classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### To get an absolute measure, use the AUC (Area under the curve)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div>\n",
    "    <img src=\"./pictures/roc.png\" width=\"700\" align=\"left\"/>\n",
    "    <br><br><br>\n",
    "    <p>Values</p>\n",
    "    <p>1.0 - Perfect</p>\n",
    "    <p>0.8 - Good</p>\n",
    "    <p>0.5 - Random</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Use Appropriate Evaluation Metrics\n",
    "\n",
    "### Precision and Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "$$Precision = \\frac{TP}{FP+TP}$$\n",
    "\n",
    "$$Recall = \\frac{TP}{FN+TP}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Recall correspond to the $True\\:Positive_{Rate}\\:(TPR)$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "<img src=\"./pictures/precision_recall.png\" width=\"600\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "_Precision_ represent how many of the positive predictions correspond to correct ones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<p style=\"font-size:0.8em;\"><b>A perfect Precision</b> means that every positive result estimated by the model engine was relevant (but says nothing about whether all the positive results were identified)</p>\n",
    "\n",
    "<br>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "\n",
    "_Recall_ represent how many of the positive class values where correctly predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<p style=\"font-size:0.8em;\"><b>A perfect Recall</b> means all positive results were identified by the model (but says nothing about how many of the positive results were incorrect)</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "One measure might be more important than the other depending on its conjecture.\n",
    "\n",
    "<br>For instance, in medical diagnostics one might prioritize Recall since is more beneficial to point out all the potential positives that it is to have a precise model but to miss positive results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## F-measure\n",
    "\n",
    "The F-measure is a harmonic mean of _Precision_ and _Recall_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$F_\\beta = (1+\\beta^2)\\frac{Precision.Recall}{(\\beta^2.Precision)+Recall}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "When _Precision_ and _Recall_ have the same weight, the metric is name the **F1-Score**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$F_1 = \\frac{2.Precision.Recall}{Precision+Recall}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\"><a href=\"https://nlp.stanford.edu/IR-book/html/htmledition/evaluation-of-unranked-retrieval-sets-1.html#10657\">Why a harmonic mean?</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2. Data level approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Sampling Methods\n",
    "<br>\n",
    "\n",
    "- Undersampling - applied to the majority class\n",
    "- Oversampling - applied to the minority class\n",
    "\n",
    "<br>\n",
    "\n",
    "Sampling methods are the most common methods for dealing with imbalanced data [4]<br>\n",
    "These methods are primarily employed to address the problem with relative rarity but do not address the issue of absolute rarity [4].\n",
    "\n",
    "<p style=\"text-align: right; font-size:0.8em\">[4] Weiss, G. (2013). Foundations of Imbalanced Learning (pg. 36)</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Undersampling\n",
    "<br>\n",
    "\n",
    "1. **[Random undersampling][1]:** random elimination of the predominant class, equalizing the class proportion;\n",
    "\n",
    "[1]: http://conteudo.icmc.usp.br/pessoas/gbatista/files/ifip2008.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "2. **[K-means undersampling][2]:** separate the predominant label in classes and remove from those the redudant examples;\n",
    "\n",
    "\n",
    "[2]: https://www.researchgate.net/publication/271913184_Undersampled_K_K_-means_approach_for_handling_imbalanced_distributed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "3. **[Tomek links][3]:** calculate the distance between two values in different classes. If the distance is the smallest between one element of this pair and any other data point from a different class, the pair is a Tomek link. In a Tomek link is safe to assume that one of the pair is either noise or the pair is at the borderline. As a undersampling technique the Tomek link technique removes the value that belongs to the majority class;\n",
    "<br><br>\n",
    "<p  style='text-align: right; font-size: 0.8em'>More techniques described in <a href=http://conteudo.icmc.usp.br/pessoas/gbatista/files/iicai2009.pdf>[5] PRATI, BATISTA and MONARD. 2009</a></p>\n",
    "\n",
    "[3]: http://conteudo.icmc.usp.br/pessoas/gbatista/files/iicai2009.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "On the other hand, random under-sampling can eventually discard data potentially important for learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Oversampling\n",
    "\n",
    "1. **[Random oversampling][1]:** is a non-heuristic method that aims to balance class distribution through the random replication of minority class examples\n",
    " \n",
    "[1]: http://conteudo.icmc.usp.br/pessoas/gbatista/files/ifip2008.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "2. **[SMOTE][2]:** _Synthetic Minority Over-sampling Technique_: the minority class is segmented using a _K-means_ algorithm. New minority samples are generated based on the difference between data a point and a few of its nearest neighbors based one the balance proportion desired. This difference is applied to each feature of the data points and are multiplied by a random number ranging from 0-1. These values are then summed to the selected data point to generate e new syntetic observation\n",
    "\n",
    "\n",
    "[2]: https://jair.org/index.php/jair/article/view/10302/24590"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "3. **[ADASYN][3]:** _Adaptive Synthetic_ Sampling Approach: uses K nearest neighbors to identify the minority class data points that are more difficult to learn (those covered with a dense majority neighborhood), the algorithm then creates the number of synthetic examples accordingly, also using the K-nearest neighbors to combine a data point with another close minority example\n",
    "<br><br>\n",
    " \n",
    "<p  style='text-align: right; font-size:0.8em'>More techniques described in <a href=http://conteudo.icmc.usp.br/pessoas/gbatista/files/iicai2009.pdf>[5] PRATI, BATISTA and MONARD. 2009</a></p>\n",
    "\n",
    "[3]: https://sci2s.ugr.es/keel/pdf/algorithm/congreso/2008-He-ieee.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3. Algorithm level approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Cost sensitive learning\n",
    "\n",
    "Cost sensitive learning produces different weights in the classification task. By shifting the probability threshold to higher or lower values one can amplify or attenuate the determination of a particular class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This is usually done, in algorithms like Relabeling, using the ratio of false predictions in the training set:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$p = \\frac{FP}{FP+FN}$$\n",
    "\n",
    "The probability threshold is then used in the test model to guarantee that only results equal or greater than this probability\n",
    "\n",
    "<p style=\"font-size: 0.8em; text-align:right\">[6] LING, C. X., SHENG, V. S. (2008). Cost-Sensitive Learning and the Class Imbalance Problem</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Emsemble methods\n",
    "\n",
    "Ensemble methods aim to leverage the classification power of multiple base learners (learned on different subsets of the training data) to improve on the classification performance over traditional classification algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    ">Under an average error rate of less than 50% and the probability of misprediction of each classifier being independent of the others, the expected error rate of an instance goes to zero as the number of classifiers goes to infinity.\n",
    "><p style=\"text-align: right; font-size: 0.8em\">[7] Hansen, L. K. & Salamon P. (1990) Neural network ensembles</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<br>\n",
    "\n",
    "**[Boosting][1]**: Create subsets of the training data, train one model and use the whole set to evaluate the model. Increase the sampling probability of the data points in which the model didn't perform as well and repeat the process for different models (e.g. AdaBoost, XGBoost)\n",
    "\n",
    "[1]: https://www.youtube.com/watch?v=GM3CDQfQ4sw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**[Bagging][2]**: Generate several subsamples and train different models subsquently (e.g. ScikitLearn's Bagging Classifier)\n",
    "\n",
    "[2]: https://www.youtube.com/watch?v=2Mg8QD0F1dQ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Many ensemble methods have been combined with sampling strategies to create ensemble methods that are more suitable for dealing with class imbalance (e.g. AdaBoost + SMOTE = SMOTEBoost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example <br><br>\n",
    "<img src=\"https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/07/McKinsey-hackathon.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Wrap up\n",
    "\n",
    "1. Identify the imbalanced proportion and analyze the relationship between features and target;\n",
    "2. Use the correct metrics and segment the problem when possible;\n",
    "3. Start small (one technique at a time), preferably using the easier ones first;\n",
    "4. If the problem requires, use an ensemble of methods to derive the result, but be carefull with explainability and performance;\n",
    "5. Always validate the model."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
